<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLINK: Languages Are New Modalities</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <article class="container">
        <header class="header">
            <h1 class="title">LLINK: Languages Are New Modalities</h1>
            <p class="subtitle">Cross-Lingual Alignment via Encoder Injection</p>
            
            <div class="meta">
                <span class="authors">Rajan Agarwal, Aarush Gupta</span>
                <span class="date">2025</span>
            </div>

            <div class="links">
                <a href="https://github.com/rajansagarwal/llink" class="link">GitHub</a>
                <span class="separator">•</span>
                <a href="#" class="link">Paper</a>
            </div>
        </header>

        <section class="content">
            <h2>Abstract</h2>
            <p>
                Instruction-tuned Large Language Models (LLMs) underperform on low-resource, non-Latin scripts due to tokenizer fragmentation and weak cross-lingual coupling. We present <strong>LLINK</strong> (Large Language Injection for Non-English Knowledge), a compute-efficient language-as-modality method that conditions an instruction-tuned decoder without changing the tokenizer or retraining the decoder. LLINK substantially improves bilingual retrieval and achieves <strong>84% preference</strong> over the base model and <strong>64% preference</strong> over direct finetuning in LLM-judged Q&A evaluations.
            </p>
        </section>

        <section class="content">
            <h2>Architecture</h2>
            <figure>
                <img src="../images/architecture.png" alt="LLINK Architecture" onerror="this.style.display='none'">
                <figcaption>LLINK treats low-resource languages as a modality, passing multilingual text through a projection model to match the LLM's embedding space.</figcaption>
            </figure>
        </section>

        <section class="content">
            <h2>Key Results</h2>
            <ul class="results-list">
                <li><strong>4.3× improvement</strong> in R@1 (from 10.4% to 45.0%) in bilingual retrieval</li>
                <li><strong>84% preference</strong> over base model in LLM-as-judge evaluation</li>
                <li><strong>3× token reduction</strong> by replacing 104 fragmented Khmer tokens with 8 semantic slots</li>
            </ul>
        </section>

        <section class="content">
            <h2>Method</h2>
            <p>LLINK uses a two-stage approach to inject low-resource language representations into decoder-only LLMs:</p>
            
            <h3>Stage A: Contrastive Alignment</h3>
            <p>Train a lightweight projection model that maps XLM-RoBERTa embeddings (768-dim) to Llama's hidden space (2048-dim) using contrastive learning with hard negative mining.</p>
            
            <h3>Stage B: Multi-Token Injection</h3>
            <p>Expand the aligned vector into K=8 soft slots and train minimal adapters so the frozen decoder learns to consume the foreign language signal.</p>
            
            <h3>The Tokenization Problem</h3>
            <p>The same sentence tokenized with Llama-3.2-1B shows severe fragmentation for non-Latin scripts:</p>
            <ul>
                <li><strong>English:</strong> 16 tokens (0.3 tok/char)</li>
                <li><strong>Khmer (Latin):</strong> 35 tokens (0.5 tok/char)</li>
                <li><strong>Khmer (Native):</strong> 104 tokens (1.7 tok/char)</li>
            </ul>
            <p>LLINK bypasses this by treating Khmer as a modality, reducing 104 tokens to just 8 semantic slots.</p>
        </section>

        <section class="content">
            <h2>Detailed Results</h2>
            
            <h3>Bilingual Retrieval Performance</h3>
            <table>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>R@1</th>
                        <th>R@5</th>
                        <th>R@10</th>
                        <th>MRR</th>
                        <th>Mean Rank</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Direct Fine-tune</td>
                        <td>0.104</td>
                        <td>0.248</td>
                        <td>0.352</td>
                        <td>0.160</td>
                        <td>24.7</td>
                    </tr>
                    <tr>
                        <td>LLINK (Stage A)</td>
                        <td>0.430</td>
                        <td>0.706</td>
                        <td>0.819</td>
                        <td>0.642</td>
                        <td>3.8</td>
                    </tr>
                    <tr class="highlight">
                        <td><strong>LLINK (Full)</strong></td>
                        <td><strong>0.450</strong></td>
                        <td><strong>0.724</strong></td>
                        <td><strong>0.835</strong></td>
                        <td><strong>0.660</strong></td>
                        <td><strong>3.4</strong></td>
                    </tr>
                </tbody>
            </table>

            <h3>LLM-as-Judge Evaluation</h3>
            <p><strong>LLINK vs. Base Model:</strong> 74 wins, 11 losses, 15 ties (87.0% preference)</p>
            <p><strong>LLINK vs. Direct Fine-tune:</strong> 49 wins, 24 losses, 27 ties (67.1% preference)</p>
        </section>

        <section class="content">
            <h2>Citation</h2>
            <pre><code>@article{agarwal2025llink,
  title={Languages Are New Modalities: Cross-Lingual Alignment via Encoder Injection},
  author={Agarwal, Rajan and Gupta, Aarush},
  journal={arXiv preprint},
  year={2025}
}</code></pre>
        </section>

        <footer>
            <p>We thank the Cohere for AI Lab, Modal team, ParaCrawl project, and the XLM-R and LLaMA teams.</p>
        </footer>
    </div>
</body>
</html>
